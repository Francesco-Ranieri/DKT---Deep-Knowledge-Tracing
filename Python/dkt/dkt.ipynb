{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dkt.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-2nTzRku2nR"
      },
      "source": [
        "!pip uninstall tensorflow\n",
        "!pip install tensorflow==2.4.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IkCyoJ_gfFP"
      },
      "source": [
        "!pip install np_utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bqWrOCrjvCGh",
        "outputId": "ac2c8ccb-9743-45f0-9e63-24a529053fb1"
      },
      "source": [
        "from tensorflow import keras\n",
        "keras.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "L7lE-hKG0ivk",
        "outputId": "b9d77940-f70b-47c2-cc19-772aa88d9c0c"
      },
      "source": [
        "import tensorflow\n",
        "tensorflow.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3UtgyoypzaW"
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0,\"/content/drive/MyDrive/Tesi\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9T6MHbJHpmlx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59433e11-a6ac-40ec-e4b4-30fb5b8a6dbf"
      },
      "source": [
        "%%writefile dkt.py\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Masking\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras import backend as K\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import random\n",
        "import math\n",
        "import argparse\n",
        "\n",
        "from tensorflow.python.keras.layers import Dense\n",
        "\n",
        "tf.compat.v1.enable_eager_execution(\n",
        "    config=None, device_policy=None, execution_mode=None\n",
        ")\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description='Process some integers.')\n",
        "    parser.add_argument('--dataset', type=str, help='Dataset file', required=True)\n",
        "    parser.add_argument('--splitfile', type=str, help='Split file', required=True)\n",
        "    parser.add_argument('--hiddenunits', type=int, help='Number of LSTM hidden units.', default=200, required=False)\n",
        "    parser.add_argument('--batchsize', type=int, help='Number of sequences to process in a batch.', default=5,\n",
        "                        required=False)\n",
        "    parser.add_argument('--timewindow', type=int, help='Number of timesteps to process in a batch.', default=100,\n",
        "                        required=False)\n",
        "    parser.add_argument('--epochs', type=int, help='Number of epochs.', default=50, required=False)\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    dataset = args.dataset\n",
        "    split_file = args.splitfile\n",
        "  # ---- MOD -----\n",
        "  # dataset = '/content/drive/MyDrive/Tesi/assistments.txt'\n",
        "  # split_file = '/content/drive/MyDrive/Tesi/assistments_split.txt'\n",
        "  # -----END -----\n",
        "\n",
        "    hidden_units = args.hiddenunits\n",
        "    batch_size = args.batchsize\n",
        "    time_window = args.timewindow\n",
        "    epochs = args.epochs\n",
        "\n",
        "    model_file = dataset + '.model_weights'\n",
        "    history_file = dataset + '.history'\n",
        "    preds_file = dataset + '.preds'\n",
        "\n",
        "    overall_loss = [0.0]\n",
        "    preds = []\n",
        "    history = []\n",
        "\n",
        "    # load dataset\n",
        "    training_seqs, testing_seqs, num_skills = load_dataset(dataset, split_file)\n",
        "    print(\"Training Sequences: %d\" % len(training_seqs))\n",
        "    print(\"Testing Sequences: %d\" % len(testing_seqs))\n",
        "    print(\"Number of skills: %d\" % num_skills)\n",
        "\n",
        "    # Our loss function\n",
        "    # The model gives predictions for all skills so we need to get the\n",
        "    # prediction for the skill at time t. We do that by taking the column-wise\n",
        "    # dot product between the predictions at each time slice and a\n",
        "    # one-hot encoding of the skill at time t.\n",
        "    # y_true: (nsamples x nsteps x nskills+1)\n",
        "    # y_pred: (nsamples x nsteps x nskills)\n",
        "    def loss_function(y_true, y_pred):\n",
        "        skill = y_true[:, :, 0:num_skills]\n",
        "        obs = y_true[:, :, num_skills]\n",
        "        rel_pred = tf.math.reduce_sum(y_pred * skill, axis=2)\n",
        "        # rel_pred = Th.sum(y_pred * skill, axis=2)\n",
        "\n",
        "        # keras implementation does a mean on the last dimension (axis=-1) which\n",
        "        # it assumes is a singleton dimension. But in our context that would\n",
        "        # be wrong.\n",
        "        return K.binary_crossentropy(rel_pred, obs)\n",
        "\n",
        "    # build model\n",
        "    model = Sequential()\n",
        "\n",
        "    # ignore padding\n",
        "    model.add(Masking(-1.0, batch_input_shape=(batch_size, time_window, num_skills * 2)))\n",
        "\n",
        "    # lstm configured to keep states between batches\n",
        "    model.add(LSTM(input_dim=num_skills * 2,\n",
        "                   units=hidden_units,\n",
        "                   return_sequences=True,\n",
        "                   batch_input_shape=(batch_size, time_window, num_skills * 2),\n",
        "                   stateful=True\n",
        "                   ))\n",
        "\n",
        "    # readout layer. TimeDistributedDense uses the same weights for all\n",
        "    # time steps.\n",
        "\n",
        "    model.add(Dense(units=num_skills, activation='sigmoid'))\n",
        "    \n",
        "    # model.add(TimeDistributedDense(input_dim=hidden_units, output_dim=num_skills, activation='sigmoid'))\n",
        "\n",
        "    # optimize with rmsprop which dynamically adapts the learning\n",
        "    # rate of each weight.\n",
        "    model.compile(loss=loss_function, optimizer='rmsprop', run_eagerly=True)  # class_mode=\"binary\"\n",
        "\n",
        "    print(model.summary())\n",
        "\n",
        "    # training function\n",
        "    def trainer(X, Y):\n",
        "        Y = tf.cast(Y, tf.float32)\n",
        "        #Y = np.asfarray(Y, float)\n",
        "        overall_loss[0] += model.train_on_batch(X, y=Y)\n",
        "\n",
        "    # prediction\n",
        "    def predictor(X, Y):\n",
        "        batch_activations = model.predict_on_batch(X)\n",
        "        skill = Y[:, :, 0:num_skills]\n",
        "        obs = Y[:, :, num_skills]\n",
        "        y_pred = np.squeeze(np.array(batch_activations))\n",
        "\n",
        "        rel_pred = np.sum(y_pred * skill, axis=2)\n",
        "\n",
        "        for b in range(0, X.shape[0]):\n",
        "            for t in range(0, X.shape[1]):\n",
        "                if X[b, t, 0] == -1.0:\n",
        "                    continue\n",
        "                preds.append((rel_pred[b][t], obs[b][t]))\n",
        "\n",
        "    # call when prediction batch is finished\n",
        "    # resets LSTM state because we are done with all sequences in the batch\n",
        "    def finished_prediction_batch(percent_done):\n",
        "        model.reset_states()\n",
        "\n",
        "    # similiar to the above\n",
        "    def finished_batch(percent_done):\n",
        "        print(\"(%4.3f %%) %f\" % (percent_done, overall_loss[0]))\n",
        "        model.reset_states()\n",
        "\n",
        "    # run the model\n",
        "    for e in range(0, epochs):\n",
        "        model.reset_states()\n",
        "\n",
        "        # train\n",
        "        run_func(training_seqs, num_skills, trainer, batch_size, time_window, finished_batch)\n",
        "\n",
        "        model.reset_states()\n",
        "\n",
        "        # test\n",
        "        run_func(testing_seqs, num_skills, predictor, batch_size, time_window, finished_prediction_batch)\n",
        "\n",
        "        # compute AUC\n",
        "        auc = roc_auc_score([p[1] for p in preds], [p[0] for p in preds])\n",
        "\n",
        "        # log\n",
        "        history.append((overall_loss[0], auc))\n",
        "\n",
        "        # save model\n",
        "        model.save_weights(model_file, overwrite=True)\n",
        "        print(\"==== Epoch: %d, Test AUC: %f\" % (e, auc))\n",
        "\n",
        "        # reset loss\n",
        "        overall_loss[0] = 0.0\n",
        "\n",
        "        # save predictions\n",
        "        with open(preds_file, 'w') as f:\n",
        "            f.write('was_heldout\\tprob_recall\\tstudent_recalled\\n')\n",
        "            for pred in preds:\n",
        "                f.write('1\\t%f\\t%d\\n' % (pred[0], pred[1]))\n",
        "\n",
        "        with open(history_file, 'w') as f:\n",
        "            for h in history:\n",
        "                f.write('\\t'.join([str(he) for he in h]))\n",
        "                f.write('\\n')\n",
        "\n",
        "        # clear preds\n",
        "        preds = []\n",
        "\n",
        "\n",
        "def run_func(seqs, num_skills, f, batch_size, time_window, batch_done=None):\n",
        "    assert (min([len(s) for s in seqs]) > 0)\n",
        "\n",
        "    # randomize samples\n",
        "    seqs = seqs[:]\n",
        "    random.shuffle(seqs)\n",
        "\n",
        "    processed = 0\n",
        "    for start_from in range(0, len(seqs), batch_size):\n",
        "        end_before = min(len(seqs), start_from + batch_size)\n",
        "        x = []\n",
        "        y = []\n",
        "        for seq in seqs[start_from:end_before]:\n",
        "            x_seq = []\n",
        "            y_seq = []\n",
        "            xt_zeros = [0 for i in range(0, num_skills * 2)]\n",
        "            ct_zeros = [0 for i in range(0, num_skills + 1)]\n",
        "            xt = xt_zeros[:]\n",
        "            for skill, is_correct in seq:\n",
        "                x_seq.append(xt)\n",
        "\n",
        "                ct = ct_zeros[:]\n",
        "                ct[skill] = 1\n",
        "                ct[num_skills] = is_correct\n",
        "                y_seq.append(ct)\n",
        "\n",
        "                # one hot encoding of (last_skill, is_correct)\n",
        "                pos = skill * 2 + is_correct\n",
        "                xt = xt_zeros[:]\n",
        "                xt[pos] = 1\n",
        "\n",
        "            x.append(x_seq)\n",
        "            y.append(y_seq)\n",
        "\n",
        "        maxlen = max([len(s) for s in x])\n",
        "        maxlen = round_to_multiple(maxlen, time_window)\n",
        "        # fill up the batch if necessary\n",
        "        if len(x) < batch_size:\n",
        "            for e in range(0, batch_size - len(x)):\n",
        "                x_seq = []\n",
        "                y_seq = []\n",
        "                for t in range(0, time_window):\n",
        "                    x_seq.append([-1.0 for i in range(0, num_skills * 2)])\n",
        "                    y_seq.append([0.0 for i in range(0, num_skills + 1)])\n",
        "                x.append(x_seq)\n",
        "                y.append(y_seq)\n",
        "\n",
        "        X = pad_sequences(x, padding='post', maxlen=maxlen, dim=num_skills * 2, value=-1.0)\n",
        "        Y = pad_sequences(y, padding='post', maxlen=maxlen, dim=num_skills + 1, value=-1.0)\n",
        "\n",
        "        for t in range(0, maxlen, time_window):\n",
        "            f(X[:, t:(t + time_window), :], Y[:, t:(t + time_window), :])\n",
        "\n",
        "        processed += end_before - start_from\n",
        "\n",
        "        # reset the states for the next batch of sequences\n",
        "        if batch_done:\n",
        "            batch_done((processed * 100.0) / len(seqs))\n",
        "\n",
        "\n",
        "def round_to_multiple(x, base):\n",
        "    return int(base * math.ceil(float(x) / base))\n",
        "\n",
        "\n",
        "def load_dataset(dataset, split_file):\n",
        "    seqs, num_skills = read_file(dataset)\n",
        "\n",
        "    with open(split_file, 'r') as f:\n",
        "        student_assignment = f.read().split(' ')\n",
        "\n",
        "    training_seqs = [seqs[i] for i in range(0, len(seqs)) if student_assignment[i] == '1']\n",
        "    testing_seqs = [seqs[i] for i in range(0, len(seqs)) if student_assignment[i] == '0']\n",
        "\n",
        "    return training_seqs, testing_seqs, num_skills\n",
        "\n",
        "\n",
        "def read_file(dataset_path):\n",
        "    seqs_by_student = {}\n",
        "    problem_ids = {}\n",
        "    next_problem_id = 0\n",
        "    with open(dataset_path, 'r') as f:\n",
        "        for line in f:\n",
        "            student, problem, is_correct = line.strip().split(' ')\n",
        "            student = int(student)\n",
        "            if student not in seqs_by_student:\n",
        "                seqs_by_student[student] = []\n",
        "            if problem not in problem_ids:\n",
        "                problem_ids[problem] = next_problem_id\n",
        "                next_problem_id += 1\n",
        "            seqs_by_student[student].append((problem_ids[problem], int(is_correct == '1')))\n",
        "\n",
        "    sorted_keys = sorted(seqs_by_student.keys())\n",
        "    return [seqs_by_student[k] for k in sorted_keys], next_problem_id\n",
        "\n",
        "\n",
        "# https://groups.google.com/forum/#!msg/keras-users/7sw0kvhDqCw/QmDMX952tq8J\n",
        "def pad_sequences(sequences, maxlen=None, dim=1, dtype='int32',\n",
        "                  padding='pre', truncating='pre', value=0.):\n",
        "    '''\n",
        "        Override keras method to allow multiple feature dimensions.\n",
        "\n",
        "        @dim: input feature dimension (number of features per timestep)\n",
        "    '''\n",
        "    lengths = [len(s) for s in sequences]\n",
        "\n",
        "    nb_samples = len(sequences)\n",
        "    if maxlen is None:\n",
        "        maxlen = np.max(lengths)\n",
        "\n",
        "    x = (np.ones((nb_samples, maxlen, dim)) * value).astype(dtype)\n",
        "    for idx, s in enumerate(sequences):\n",
        "        if truncating == 'pre':\n",
        "            trunc = s[-maxlen:]\n",
        "        elif truncating == 'post':\n",
        "            trunc = s[:maxlen]\n",
        "        else:\n",
        "            raise ValueError(\"Truncating type '%s' not understood\" % padding)\n",
        "\n",
        "        if padding == 'post':\n",
        "            x[idx, :len(trunc)] = trunc\n",
        "        elif padding == 'pre':\n",
        "            x[idx, -len(trunc):] = trunc\n",
        "        else:\n",
        "            raise ValueError(\"Padding type '%s' not understood\" % padding)\n",
        "    return x\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing dkt.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIlwDKgpun7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16e2af52-0016-47b3-a0cd-5ea5483a01d8"
      },
      "source": [
        "!python dkt.py \"--dataset\" /content/drive/MyDrive/Tesi/assistments.txt \"--splitfile\" /content/drive/MyDrive/Tesi/assistments_split.txt \"--epochs\" 10"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-30 15:45:34.798668: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "Training Sequences: 3361\n",
            "Testing Sequences: 856\n",
            "Number of skills: 124\n",
            "2021-05-30 15:45:36.603687: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
            "2021-05-30 15:45:36.630604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-30 15:45:36.631174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2021-05-30 15:45:36.631215: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-05-30 15:45:36.633675: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
            "2021-05-30 15:45:36.633760: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
            "2021-05-30 15:45:36.635385: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
            "2021-05-30 15:45:36.635716: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
            "2021-05-30 15:45:36.637273: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-05-30 15:45:36.637975: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
            "2021-05-30 15:45:36.638153: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
            "2021-05-30 15:45:36.638246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-30 15:45:36.638836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-30 15:45:36.639353: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
            "2021-05-30 15:45:36.639828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-30 15:45:36.640372: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2021-05-30 15:45:36.640451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-30 15:45:36.641004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-30 15:45:36.641511: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
            "2021-05-30 15:45:36.641562: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-05-30 15:45:37.231877: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-05-30 15:45:37.231931: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
            "2021-05-30 15:45:37.231952: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
            "2021-05-30 15:45:37.232126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-30 15:45:37.232760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-30 15:45:37.233374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-30 15:45:37.233877: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-05-30 15:45:37.233924: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13837 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "masking (Masking)            (5, 100, 248)             0         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (5, 100, 200)             359200    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (5, 100, 124)             24924     \n",
            "=================================================================\n",
            "Total params: 384,124\n",
            "Trainable params: 384,124\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "2021-05-30 15:45:37.889558: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
            "2021-05-30 15:45:38.373560: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8004\n",
            "2021-05-30 15:45:38.556073: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
            "2021-05-30 15:45:39.016551: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
            "(0.149 %) 9.485503\n",
            "(0.298 %) 12.533941\n",
            "(0.446 %) 14.614169\n",
            "(0.595 %) 17.988567\n",
            "(0.744 %) 22.540455\n",
            "(0.893 %) 43.589699\n",
            "(1.041 %) 47.257066\n",
            "(1.190 %) 50.414194\n",
            "(1.339 %) 58.744855\n",
            "(1.488 %) 67.991867\n",
            "(1.636 %) 84.180399\n",
            "(1.785 %) 84.746097\n",
            "(1.934 %) 96.508025\n",
            "(2.083 %) 144.048378\n",
            "(2.231 %) 149.053486\n",
            "(2.380 %) 155.771237\n",
            "(2.529 %) 166.153582\n",
            "(2.678 %) 178.246443\n",
            "(2.827 %) 183.359088\n",
            "(2.975 %) 195.285083\n",
            "(3.124 %) 199.916255\n",
            "(3.273 %) 204.833557\n",
            "(3.422 %) 213.868403\n",
            "(3.570 %) 215.832972\n",
            "(3.719 %) 219.981858\n",
            "(3.868 %) 222.190529\n",
            "(4.017 %) 228.961320\n",
            "(4.165 %) 230.008175\n",
            "(4.314 %) 231.034024\n",
            "(4.463 %) 246.806935\n",
            "(4.612 %) 248.472148\n",
            "(4.760 %) 250.949042\n",
            "(4.909 %) 271.710900\n",
            "(5.058 %) 274.602223\n",
            "(5.207 %) 285.003386\n",
            "(5.356 %) 301.836656\n",
            "(5.504 %) 312.260945\n",
            "(5.653 %) 354.459272\n",
            "(5.802 %) 358.821688\n",
            "(5.951 %) 366.568892\n",
            "(6.099 %) 369.728185\n",
            "(6.248 %) 371.619972\n",
            "(6.397 %) 376.493441\n",
            "(6.546 %) 382.986173\n",
            "(6.694 %) 392.709029\n",
            "(6.843 %) 401.309598\n",
            "(6.992 %) 402.721212\n",
            "(7.141 %) 409.262716\n",
            "(7.289 %) 416.154854\n",
            "(7.438 %) 423.442697\n",
            "(7.587 %) 423.719994\n",
            "(7.736 %) 428.753436\n",
            "(7.885 %) 432.120039\n",
            "(8.033 %) 438.965758\n",
            "(8.182 %) 440.061640\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}